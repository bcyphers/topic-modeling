{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling with nltk and scikit-learn\n",
    "\n",
    "This will be a quick walk-through of building a topic model. I'm using [scikit-learn](http://scikit-learn.org/stable/), [nltk](http://www.nltk.org/) and [pandas](https://pandas.pydata.org/).\n",
    "\n",
    "First, load some data. I'm using a cleaned version of the Pitchfork music review dataset from [kaggle](https://www.kaggle.com/nolanbconaway/pitchfork-data) that's hosted on my website. It has 18 thousand reviews with about 12 million words total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18378 documents\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "DATA_PATH = 'reviews.csv'\n",
    "\n",
    "# download the data if necessary\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    f = urllib2.urlopen('https://cyphe.rs/static/reviews.csv')\n",
    "    with open(DATA_PATH, 'wb') as outfile:\n",
    "        outfile.write(f.read())\n",
    "    \n",
    "# load it into a dataframe  \n",
    "df = pd.read_csv('reviews.csv')\n",
    "\n",
    "\n",
    "# we only care about the content of the reviews.\n",
    "# filter out ones None/NaN content.\n",
    "df = df[[type(c) == str for c in df.content]]\n",
    "docs = df.content\n",
    "print len(docs), 'documents'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the raw text\n",
    "\n",
    "First, create a tokenizer. This will crawl through the raw text and convert it to a series of cleaned tokens.\n",
    "\n",
    "You can use a preconfigured tokenizer, or define one using a regex. The example below is very simple, but it works well enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# this will find all alphanumeric tokens of 2 or more characters.\n",
    "# it will not count hyphenated words or contractions as one token.\n",
    "tokenizer = RegexpTokenizer('\\w\\w+').tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize the documents\n",
    "\n",
    "Now create a vectorizer. This will convert the set of tokens for each document into a vector of constant size so that documents can be analyzed with linear algebra. \n",
    "\n",
    "There are a variety of vectorization methods you can use. Most perform some variation of counting the most common few thousand words in each document. [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) (term frequency -- inverse document frequency) is probably the most popular method. TF-IDF deemphasizes words that are common in every document, so it tends to be better for topic modeling. You can also try a count vectorizer (which just uses unprocessed word counts) or a hashing vectorizer (which maps  all words to the vector space, not just the N most common). Keep in mind that using a hashing vectorizer will make it much more difficult to convert topics generated by the final model into human-interpretable topics.\n",
    "\n",
    "The important parameters here are `max_df` (which determines how many common words are allowed into your topics), `max_features` (which determines the size of the vectors), and `stop_words` (if set to \"english\", common words like \"the\", \"and\", etc. will be ignored)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# min_df and max_df control which tokens are considered by the vectorizer.\n",
    "# only words which are in at least min_df documents but not in more than max_df documents are counted.\n",
    "# if either value is < 1, it represents a portion of the total documents.\n",
    "\n",
    "# max_features is the size of the vector -- the total number of distinct words which the\n",
    "# vectorizer counts for each document.\n",
    "vectorizer = TfidfVectorizer(max_df=0.6,\n",
    "                             min_df=2,\n",
    "                             max_features=1000,\n",
    "                             tokenizer=tokenizer,\n",
    "                             stop_words='english')\n",
    "\n",
    "doc_vecs = vectorizer.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a topic model\n",
    "\n",
    "Finally, use the vectorized documents train a topic model. At this point, each document is represented by a 1000-dimensional TF/IDF vector, and all the doc vectors form a `(n_docs x n_features)` matrix. A topic model will attempt to find an approximate factorization of the matrix that retains most of the relevant information.\n",
    "\n",
    "SKlearn implements two methods, latent Dirichlet allocation (LDA) and non-negative matrix factorization (NMF). They basically solve the same problem, but LDA is supposed to be qualitatively better for big-data settings. I've had better luck with NMF for datasets around this size. This [quora post](https://www.quora.com/What-is-the-difference-between-NMF-and-LDA-Why-are-the-priors-of-LDA-sparse-induced) goes into more detail about the differences.\n",
    "\n",
    "The main parameter choice you have to make is the number of topics you want. Too many, and you might have lots of near-duplicates; too few, and you might miss important themes in the data. You can try a few different values and see what works.\n",
    "\n",
    "Each \"topic\" is a linear combination of all the features created by the vectorizer, and each feature corresponds to a token. Topics tend to be dominated by just a few features, so the topics can be thought of as weighted groups of words that tend to occur together. The utility functions below convert the raw topic vectors into readable strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF #, LatentDirichletAllocation\n",
    "\n",
    "def name_topics(vectorizer, model):\n",
    "    \"\"\"\n",
    "    Map topic vectors to human-readable groups of words, and save the data as \n",
    "    a dataframe with named columns.\n",
    "    \"\"\"\n",
    "    topics = []\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    for comp in model.components_:\n",
    "        total = sum(comp)\n",
    "        topic = ', '.join('(%.3f) %s' % (comp[i] / total, feature_names[i])\n",
    "                          for i in comp.argsort()[:-6:-1])\n",
    "        topics.append(topic)\n",
    "    return topics\n",
    "\n",
    "def print_top_topics(model, topics, vectors):\n",
    "    sums = vectors.sum(axis=0)\n",
    "    print 'Top topics:'\n",
    "    for i in np.argsort(-sums):\n",
    "        print '%.2f:' % sums[i], topics[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try with just a few topics first. This should train very quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top topics:\n",
      "512.86: (0.009) guitar, (0.007) sounds, (0.007) work, (0.006) track, (0.006) piano\n",
      "479.83: (0.009) love, (0.009) pop, (0.006) don, (0.005) life, (0.005) good\n",
      "374.92: (0.040) band, (0.019) rock, (0.012) metal, (0.011) punk, (0.009) bands\n",
      "326.55: (0.018) house, (0.018) dance, (0.013) techno, (0.012) disco, (0.012) tracks\n",
      "234.34: (0.028) rap, (0.022) hop, (0.021) hip, (0.012) rapper, (0.009) beats\n"
     ]
    }
   ],
   "source": [
    "model5 = NMF(n_components=5)\n",
    "topic_vecs = model5.fit_transform(doc_vecs)\n",
    "topics5 = name_topics(vectorizer, model5)\n",
    "print_top_topics(model5, topics5, topic_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More topics (will take a little longer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top topics:\n",
      "323.19: (0.013) don, (0.013) good, (0.012) know, (0.012) really, (0.011) ve\n",
      "297.86: (0.014) guitar, (0.012) noise, (0.011) track, (0.010) sounds, (0.009) electronic\n",
      "282.48: (0.015) record, (0.012) feels, (0.010) work, (0.010) feel, (0.009) sense\n",
      "272.28: (0.021) live, (0.017) disc, (0.013) set, (0.012) version, (0.011) tracks\n",
      "252.57: (0.122) band, (0.011) group, (0.010) members, (0.008) ve, (0.008) drummer\n",
      "221.39: (0.122) rock, (0.028) indie, (0.021) roll, (0.019) guitar, (0.017) bands\n",
      "210.25: (0.088) pop, (0.023) indie, (0.010) synth, (0.009) melodies, (0.008) debut\n",
      "186.15: (0.041) house, (0.036) dance, (0.031) techno, (0.020) dj, (0.019) disco\n",
      "180.61: (0.031) folk, (0.021) country, (0.017) guitar, (0.017) acoustic, (0.017) voice\n",
      "166.96: (0.107) punk, (0.030) post, (0.029) hardcore, (0.015) wave, (0.015) new\n",
      "161.02: (0.093) love, (0.014) sings, (0.014) life, (0.012) girl, (0.012) heart\n",
      "145.66: (0.065) rap, (0.029) rapper, (0.017) mixtape, (0.012) beats, (0.009) young\n",
      "145.32: (0.109) ep, (0.023) track, (0.014) release, (0.014) new, (0.014) year\n",
      "138.27: (0.068) funk, (0.062) soul, (0.020) disco, (0.015) brown, (0.014) james\n",
      "138.00: (0.226) black, (0.029) white, (0.014) mountain, (0.013) man, (0.012) blues\n",
      "134.73: (0.101) hop, (0.096) hip, (0.018) beats, (0.014) beat, (0.013) samples\n",
      "129.01: (0.096) jazz, (0.023) free, (0.015) group, (0.013) musicians, (0.013) playing\n",
      "111.53: (0.088) film, (0.053) soundtrack, (0.028) movie, (0.012) work, (0.010) strings\n",
      "92.83: (0.147) metal, (0.034) death, (0.026) doom, (0.016) riffs, (0.015) heavy\n",
      "58.78: (0.282) smith, (0.036) fall, (0.011) mark, (0.005) tribute, (0.005) night\n"
     ]
    }
   ],
   "source": [
    "model20 = NMF(n_components=20)\n",
    "topic_vecs = model20.fit_transform(doc_vecs)\n",
    "topics20 = name_topics(vectorizer, model20)\n",
    "print_top_topics(model20, topics20, topic_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tons of topics (might take a while):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top topics:\n",
      "216.76: (0.038) life, (0.028) man, (0.026) world, (0.014) self, (0.010) people\n",
      "201.36: (0.083) track, (0.055) tracks, (0.027) title, (0.026) minutes, (0.019) minute\n",
      "198.37: (0.016) don, (0.016) good, (0.015) really, (0.015) ve, (0.014) know\n",
      "183.15: (0.057) voice, (0.024) sings, (0.023) lyrics, (0.021) singing, (0.021) singer\n",
      "178.41: (0.026) electronic, (0.023) sounds, (0.023) ambient, (0.019) piece, (0.017) pieces\n",
      "169.07: (0.026) feels, (0.024) feel, (0.017) sense, (0.017) way, (0.017) work\n",
      "167.65: (0.202) band, (0.013) members, (0.011) bands, (0.011) ve, (0.011) drummer\n",
      "165.86: (0.028) years, (0.024) albums, (0.019) released, (0.018) label, (0.017) career\n",
      "164.13: (0.107) guitar, (0.028) acoustic, (0.022) guitars, (0.021) electric, (0.016) vocals\n",
      "162.94: (0.228) new, (0.064) york, (0.043) wave, (0.028) old, (0.024) age\n",
      "153.10: (0.064) night, (0.043) sun, (0.032) light, (0.031) moon, (0.024) blue\n",
      "143.58: (0.068) bass, (0.043) drum, (0.028) beat, (0.028) beats, (0.024) drums\n",
      "136.20: (0.222) record, (0.038) records, (0.011) lp, (0.010) moments, (0.010) production\n",
      "135.81: (0.209) rock, (0.038) roll, (0.023) garage, (0.018) classic, (0.017) post\n",
      "128.13: (0.243) love, (0.021) heart, (0.020) girl, (0.015) romantic, (0.012) want\n",
      "127.27: (0.136) piano, (0.036) strings, (0.014) melody, (0.014) string, (0.013) arrangements\n",
      "125.11: (0.134) live, (0.036) studio, (0.027) set, (0.026) performance, (0.022) audience\n",
      "124.27: (0.174) group, (0.018) members, (0.013) collective, (0.010) trio, (0.009) member\n",
      "121.48: (0.154) indie, (0.047) bands, (0.016) scene, (0.011) rock, (0.010) self\n",
      "117.14: (0.228) young, (0.022) old, (0.016) beach, (0.015) girl, (0.014) youth\n",
      "109.95: (0.144) folk, (0.031) acoustic, (0.018) traditional, (0.015) psych, (0.013) american\n",
      "108.81: (0.244) disco, (0.030) synth, (0.021) electro, (0.019) 80s, (0.016) space\n",
      "107.01: (0.099) remix, (0.078) original, (0.041) version, (0.034) mix, (0.027) material\n",
      "106.99: (0.170) punk, (0.047) post, (0.044) hardcore, (0.022) bands, (0.013) wave\n",
      "105.23: (0.172) pop, (0.014) power, (0.014) synth, (0.014) hooks, (0.013) chorus\n",
      "102.69: (0.370) black, (0.021) mountain, (0.011) dark, (0.008) roots, (0.007) cover\n",
      "99.10: (0.154) duo, (0.030) ghost, (0.017) pair, (0.017) debut, (0.016) ve\n",
      "95.95: (0.129) disc, (0.028) set, (0.023) compilation, (0.020) tracks, (0.019) box\n",
      "92.11: (0.160) country, (0.022) american, (0.020) steel, (0.014) brothers, (0.013) old\n",
      "88.62: (0.215) house, (0.053) dj, (0.033) mix, (0.025) acid, (0.023) chicago\n",
      "88.43: (0.136) hop, (0.130) hip, (0.018) beats, (0.015) dj, (0.015) samples\n",
      "87.27: (0.266) city, (0.037) york, (0.026) chicago, (0.023) girls, (0.021) scene\n",
      "86.19: (0.236) ep, (0.026) year, (0.026) release, (0.021) length, (0.015) released\n",
      "81.33: (0.150) jazz, (0.034) free, (0.016) playing, (0.014) musicians, (0.014) trio\n",
      "77.72: (0.200) dance, (0.040) club, (0.025) party, (0.020) floor, (0.015) electronic\n",
      "77.04: (0.270) death, (0.022) dead, (0.014) blood, (0.014) dark, (0.013) ride\n",
      "75.91: (0.208) james, (0.126) brown, (0.044) morning, (0.021) acid, (0.010) king\n",
      "75.09: (0.113) funk, (0.112) soul, (0.019) prince, (0.018) 70s, (0.016) groove\n",
      "74.96: (0.098) lo, (0.097) fi, (0.020) recording, (0.020) tape, (0.019) garage\n",
      "74.43: (0.156) techno, (0.031) label, (0.023) minimal, (0.020) dub, (0.020) mix\n",
      "67.59: (0.188) metal, (0.032) doom, (0.020) riffs, (0.018) heavy, (0.016) hardcore\n",
      "67.25: (0.142) film, (0.084) soundtrack, (0.044) movie, (0.013) work, (0.013) theme\n",
      "67.22: (0.309) la, (0.020) french, (0.017) girls, (0.013) girl, (0.010) oh\n",
      "63.57: (0.126) noise, (0.025) sonic, (0.022) drone, (0.017) feedback, (0.015) youth\n",
      "63.54: (0.089) rap, (0.040) rapper, (0.022) mixtape, (0.016) beats, (0.011) production\n",
      "58.73: (0.307) white, (0.016) boy, (0.013) red, (0.012) hot, (0.011) girl\n",
      "52.87: (0.345) smith, (0.044) fall, (0.013) mark, (0.006) tribute, (0.005) occasionally\n",
      "50.67: (0.242) blues, (0.011) playing, (0.011) john, (0.010) american, (0.008) baby\n",
      "45.44: (0.287) jones, (0.011) mike, (0.009) big, (0.008) tom, (0.008) soul\n",
      "44.45: (0.180) dylan, (0.016) war, (0.015) beatles, (0.014) american, (0.012) covers\n"
     ]
    }
   ],
   "source": [
    "model50 = NMF(n_components=50)\n",
    "topic_vecs = model50.fit_transform(doc_vecs)\n",
    "topics50 = name_topics(vectorizer, model50)\n",
    "print_top_topics(model50, topics50, topic_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can put the vectorizer and the topic model together into a pipeline and classify arbitrary documents with the new model. You can use the topics as features for classificaiton, or to cluster documents together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.039: (0.031) folk, (0.021) country, (0.017) guitar, (0.017) acoustic, (0.017) voice\n",
      "0.038: (0.107) punk, (0.030) post, (0.029) hardcore, (0.015) wave, (0.015) new\n",
      "0.030: (0.013) don, (0.013) good, (0.012) know, (0.012) really, (0.011) ve\n",
      "0.029: (0.122) band, (0.011) group, (0.010) members, (0.008) ve, (0.008) drummer\n",
      "0.017: (0.122) rock, (0.028) indie, (0.021) roll, (0.019) guitar, (0.017) bands\n",
      "\n",
      "\"flaws\" by bombay bicycle club (rock)\n",
      "\n",
      "Bombay Bicycle Club's debut album, last year's I Had the Blues But I Shook Them Loose, posited the group as imitative but energetic post-punk revivalists, doling out artily precise pop-rockers with liveliness and strong chops. The band now follows that effort with its all-acoustic sophomore long-player, Flaws.Wait, did I miss a few steps? Since when does a boisterous young rock group make a full-on introspective acoustic move on its second freaking album? This may come as a shock, but Bombay Bicycle Club didn't exactly master its initial sound on its debut (most bands don't manage that feat in just one album). The group's members are barely out of their teens, so perhaps the fickleness of youth explains their inconstancy. In any event, it seems that between records someone fell hard for the pretty folk of Nick Drake, John Martyn, and Joanna Newsom. And by \"someone\" I mean \"probably lead singer Jack Steadman,\" given the extent to which his words and voice now dominate the proceedings. Kind of makes you wonder how on-board the rest of the band were with this abrupt stylistic shift.Taking a folkier turn might make sense on paper considering Steadman's voice possesses a nicely quavering element that isn't far off from Devendra Banhart's. Yet the lyrics and arrangments here aren't a fraction as bold or as singular. Flaws is well-produced, many of its songs nicely augmented by fleet drumming and intricate guitar figures, but Steadman's lack of having anything interesting to say and inability to say it distinctively ultimately sinks the endeavor. Sure, the guitars on \"Rinse Me Down\" evoke Out of Time-era R.E.M. while \"Many Ways\" offers pleasant Sufjan-ish banjos, but Steadman replaces the incision of those artists with empty floridity and moody romanticism. The group covers Martyn here, and it's telling that they choose the juvenilia of \"Fairytale Lullaby\". If Steadman and his mates are really so dead-set on being sober young\r\n",
      "Listening to Flaws and knowing what a spirited racket these kids are capable of making, I want to grab all of them by the cheeks like Adam Sandler does to his chubby third-grade classmate in Billy Madison and yell, \"For the love of God, you have to cherish it!\" There will be plenty of time to become pussies later on down the road.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pipeline = make_pipeline(vectorizer, model20)\n",
    "\n",
    "def rank_topics(topics, vec):\n",
    "    for i in np.argsort(-vec)[:5]:\n",
    "        print '%.3f: %s' % (vec[i], topics[i])\n",
    "\n",
    "example = df.iloc[np.random.randint(len(df))]\n",
    "        \n",
    "top_vec = pipeline.transform([example.content])\n",
    "rank_topics(topics20, top_vec[0])\n",
    "\n",
    "print\n",
    "print '\"%s\" by %s (%s)' % (example.title, example.artist, example.genre)\n",
    "print\n",
    "print example.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
