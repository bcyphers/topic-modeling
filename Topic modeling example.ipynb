{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling with nltk and scikit-learn\n",
    "\n",
    "This will be a quick walk-through of building a topic model. I'm using [scikit-learn](http://scikit-learn.org/stable/), [nltk](http://www.nltk.org/) and [pandas](https://pandas.pydata.org/).\n",
    "\n",
    "First, load some data. I'm using a cleaned version of the Pitchfork music review dataset from [kaggle](https://www.kaggle.com/nolanbconaway/pitchfork-data) that's hosted on my website. It has 18 thousand reviews with about 12 million words total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18378 documents\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "DATA_PATH = 'reviews.csv'\n",
    "\n",
    "# download the data if necessary\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    f = urllib2.urlopen('https://cyphe.rs/static/reviews.csv')\n",
    "    with open(DATA_PATH, 'wb') as outfile:\n",
    "        outfile.write(f.read())\n",
    "    \n",
    "# load it into a dataframe  \n",
    "df = pd.read_csv('reviews.csv')\n",
    "\n",
    "\n",
    "# we only care about the content of the reviews.\n",
    "# filter out ones None/NaN content.\n",
    "df = df[[type(c) == str for c in df.content]]\n",
    "docs = df.content\n",
    "print len(docs), 'documents'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the raw text\n",
    "\n",
    "First, create a tokenizer. This will crawl through the raw text and convert it to a series of cleaned tokens.\n",
    "\n",
    "You can use a preconfigured tokenizer, or define one using a regex. The example below is very simple, but it works well enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# this will find all alphanumeric tokens of 2 or more characters.\n",
    "# it will not count hyphenated words or contractions as one token.\n",
    "tokenizer = RegexpTokenizer('\\w\\w+').tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize the documents\n",
    "\n",
    "Now create a vectorizer. This will convert the set of tokens for each document into a vector of constant size so that documents can be analyzed with linear algebra. \n",
    "\n",
    "There are a variety of vectorization methods you can use. Most perform some variation of counting the most common few thousand words in each document. [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) (term frequency -- inverse document frequency) is probably the most popular method. TF-IDF deemphasizes words that are common in every document, so it tends to be better for topic modeling. You can also try a count vectorizer (which just uses unprocessed word counts) or a hashing vectorizer (which maps  all words to the vector space, not just the N most common). Keep in mind that using a hashing vectorizer will make it much more difficult to convert topics generated by the final model into human-interpretable topics.\n",
    "\n",
    "The important parameters here are `max_df` (which determines how many common words are allowed into your topics), `max_features` (which determines the size of the vectors), and `stop_words` (if set to \"english\", common words like \"the\", \"and\", etc. will be ignored)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# min_df and max_df control which tokens are considered by the vectorizer.\n",
    "# only words which are in at least min_df documents but not in more than max_df documents are counted.\n",
    "# if either value is < 1, it represents a portion of the total documents.\n",
    "\n",
    "# max_features is the size of the vector -- the total number of distinct words which the\n",
    "# vectorizer counts for each document.\n",
    "vectorizer = TfidfVectorizer(max_df=0.6,\n",
    "                             min_df=2,\n",
    "                             max_features=1000,\n",
    "                             tokenizer=tokenizer,\n",
    "                             stop_words='english')\n",
    "\n",
    "doc_vecs = vectorizer.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a topic model\n",
    "\n",
    "Finally, use the vectorized documents train a topic model. At this point, each document is represented by a 1000-dimensional TF/IDF vector, and all the doc vectors form a `(n_docs x n_features)` matrix. A topic model will attempt to find an approximate factorization of the matrix that retains most of the relevant information.\n",
    "\n",
    "SKlearn implements two methods, latent Dirichlet allocation (LDA) and non-negative matrix factorization (NMF). They basically solve the same problem, but LDA is supposed to be qualitatively better for big-data settings. I've had better luck with NMF for datasets around this size. This [quora post](https://www.quora.com/What-is-the-difference-between-NMF-and-LDA-Why-are-the-priors-of-LDA-sparse-induced) goes into more detail about the differences.\n",
    "\n",
    "The main parameter choice you have to make is the number of topics you want. Too many, and you might have lots of near-duplicates; too few, and you might miss important themes in the data. You can try a few different values and see what works.\n",
    "\n",
    "Each \"topic\" is a linear combination of all the features created by the vectorizer, and each feature corresponds to a token. Topics tend to be dominated by just a few features, so the topics can be thought of as weighted groups of words that tend to occur together. The utility functions below convert the raw topic vectors into readable strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF #, LatentDirichletAllocation\n",
    "\n",
    "def name_topics(vectorizer, model):\n",
    "    \"\"\"\n",
    "    Map topic vectors to human-readable groups of words, and save the data as \n",
    "    a dataframe with named columns.\n",
    "    \"\"\"\n",
    "    topics = []\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    for comp in model.components_:\n",
    "        total = sum(comp)\n",
    "        topic = ', '.join('(%.3f) %s' % (comp[i] / total, feature_names[i])\n",
    "                          for i in comp.argsort()[:-6:-1])\n",
    "        topics.append(topic)\n",
    "    return topics\n",
    "\n",
    "def print_top_topics(model, topics, vectors):\n",
    "    sums = vectors.sum(axis=0)\n",
    "    print 'Top topics:'\n",
    "    for i in np.argsort(-sums):\n",
    "        print '%.2f:' % sums[i], topics[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try with just a few topics first. This should train very quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top topics:\n",
      "512.86: (0.009) guitar, (0.007) sounds, (0.007) work, (0.006) track, (0.006) piano\n",
      "479.83: (0.009) love, (0.009) pop, (0.006) don, (0.005) life, (0.005) good\n",
      "374.92: (0.040) band, (0.019) rock, (0.012) metal, (0.011) punk, (0.009) bands\n",
      "326.55: (0.018) house, (0.018) dance, (0.013) techno, (0.012) disco, (0.012) tracks\n",
      "234.34: (0.028) rap, (0.022) hop, (0.021) hip, (0.012) rapper, (0.009) beats\n"
     ]
    }
   ],
   "source": [
    "model5 = NMF(n_components=5)\n",
    "topic_vecs = model5.fit_transform(doc_vecs)\n",
    "topics5 = name_topics(vectorizer, model5)\n",
    "print_top_topics(model5, topics5, topic_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More topics (will take a little longer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top topics:\n",
      "323.19: (0.013) don, (0.013) good, (0.012) know, (0.012) really, (0.011) ve\n",
      "297.86: (0.014) guitar, (0.012) noise, (0.011) track, (0.010) sounds, (0.009) electronic\n",
      "282.48: (0.015) record, (0.012) feels, (0.010) work, (0.010) feel, (0.009) sense\n",
      "272.28: (0.021) live, (0.017) disc, (0.013) set, (0.012) version, (0.011) tracks\n",
      "252.57: (0.122) band, (0.011) group, (0.010) members, (0.008) ve, (0.008) drummer\n",
      "221.39: (0.122) rock, (0.028) indie, (0.021) roll, (0.019) guitar, (0.017) bands\n",
      "210.25: (0.088) pop, (0.023) indie, (0.010) synth, (0.009) melodies, (0.008) debut\n",
      "186.15: (0.041) house, (0.036) dance, (0.031) techno, (0.020) dj, (0.019) disco\n",
      "180.61: (0.031) folk, (0.021) country, (0.017) guitar, (0.017) acoustic, (0.017) voice\n",
      "166.96: (0.107) punk, (0.030) post, (0.029) hardcore, (0.015) wave, (0.015) new\n",
      "161.02: (0.093) love, (0.014) sings, (0.014) life, (0.012) girl, (0.012) heart\n",
      "145.66: (0.065) rap, (0.029) rapper, (0.017) mixtape, (0.012) beats, (0.009) young\n",
      "145.32: (0.109) ep, (0.023) track, (0.014) release, (0.014) new, (0.014) year\n",
      "138.27: (0.068) funk, (0.062) soul, (0.020) disco, (0.015) brown, (0.014) james\n",
      "138.00: (0.226) black, (0.029) white, (0.014) mountain, (0.013) man, (0.012) blues\n",
      "134.73: (0.101) hop, (0.096) hip, (0.018) beats, (0.014) beat, (0.013) samples\n",
      "129.01: (0.096) jazz, (0.023) free, (0.015) group, (0.013) musicians, (0.013) playing\n",
      "111.53: (0.088) film, (0.053) soundtrack, (0.028) movie, (0.012) work, (0.010) strings\n",
      "92.83: (0.147) metal, (0.034) death, (0.026) doom, (0.016) riffs, (0.015) heavy\n",
      "58.78: (0.282) smith, (0.036) fall, (0.011) mark, (0.005) tribute, (0.005) night\n"
     ]
    }
   ],
   "source": [
    "model20 = NMF(n_components=20)\n",
    "topic_vecs = model20.fit_transform(doc_vecs)\n",
    "topics20 = name_topics(vectorizer, model20)\n",
    "print_top_topics(model20, topics20, topic_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tons of topics (might take a while):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top topics:\n",
      "296.08: (0.014) noise, (0.014) guitar, (0.011) track, (0.010) electronic, (0.010) sounds\n",
      "273.76: (0.014) don, (0.013) good, (0.012) know, (0.012) really, (0.012) ve\n",
      "270.94: (0.021) record, (0.013) feels, (0.012) feel, (0.012) work, (0.010) sense\n",
      "250.19: (0.126) band, (0.011) group, (0.010) members, (0.008) ve, (0.008) bands\n",
      "238.04: (0.028) live, (0.021) disc, (0.016) set, (0.015) version, (0.015) tracks\n",
      "225.36: (0.035) new, (0.018) years, (0.015) old, (0.015) young, (0.014) city\n",
      "210.06: (0.093) pop, (0.024) indie, (0.011) synth, (0.009) melodies, (0.008) group\n",
      "208.04: (0.086) love, (0.015) sings, (0.013) life, (0.012) soul, (0.012) heart\n",
      "204.14: (0.128) rock, (0.029) indie, (0.022) roll, (0.020) guitar, (0.018) bands\n",
      "188.75: (0.041) house, (0.036) dance, (0.030) techno, (0.023) disco, (0.020) dj\n",
      "180.13: (0.031) folk, (0.019) guitar, (0.018) country, (0.017) acoustic, (0.017) voice\n",
      "150.12: (0.086) jazz, (0.028) funk, (0.020) soul, (0.020) free, (0.011) musicians\n",
      "143.20: (0.113) ep, (0.023) track, (0.015) release, (0.015) year, (0.014) new\n",
      "135.23: (0.098) hop, (0.092) hip, (0.018) beats, (0.015) beat, (0.012) samples\n",
      "132.55: (0.070) rap, (0.032) rapper, (0.018) mixtape, (0.013) beats, (0.009) production\n",
      "127.46: (0.124) punk, (0.035) post, (0.033) hardcore, (0.015) bands, (0.014) wave\n",
      "120.20: (0.233) black, (0.031) white, (0.014) mountain, (0.014) blues, (0.012) man\n",
      "110.91: (0.090) film, (0.054) soundtrack, (0.028) movie, (0.014) work, (0.010) strings\n",
      "92.22: (0.149) metal, (0.035) death, (0.027) doom, (0.016) riffs, (0.015) heavy\n",
      "60.82: (0.288) smith, (0.037) fall, (0.012) mark, (0.005) night, (0.005) tribute\n"
     ]
    }
   ],
   "source": [
    "model50 = NMF(n_components=50)\n",
    "topic_vecs = model50.fit_transform(doc_vecs)\n",
    "topics50 = name_topics(vectorizer, model50)\n",
    "print_top_topics(model50, topics50, topic_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can put the vectorizer and the topic model together into a pipeline and classify arbitrary documents with the new model. You can use the topics as features for classificaiton, or to cluster documents together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.040: (0.014) guitar, (0.012) noise, (0.011) track, (0.010) sounds, (0.009) electronic\n",
      "0.038: (0.041) house, (0.036) dance, (0.031) techno, (0.020) dj, (0.019) disco\n",
      "0.036: (0.101) hop, (0.096) hip, (0.018) beats, (0.014) beat, (0.013) samples\n",
      "0.034: (0.109) ep, (0.023) track, (0.014) release, (0.014) new, (0.014) year\n",
      "0.024: (0.015) record, (0.012) feels, (0.010) work, (0.010) feel, (0.009) sense\n",
      "\n",
      "\"lp\" by container (experimental)\n",
      "\n",
      "Ren Schofield was a member of the noise scene before he started his one-man techno project Container, and you can hear this anarchic influence seep into the clean lines of his current work. \"I like things raw and kind of sloppy,\" he told Resident Advisor in 2012. \"I like things when they're not perfect.\" That sensibility is what makes Container’s records so compelling and unique. His songs live on the verge of chaos, and though they never actually fall apart, the threat remains. At the same time, his adherence to regular rhythms and logical changes makes each track focused and orderly, at least compared to abstract noise. So you can dance to Container’s music without falling over, but you can also chill to it without falling asleep.The mix that Schofield has hit on is so potent that he hasn’t needed to change it up much so far. There’s not much difference between this new album and his previous two full-length efforts, which perhaps explains why he gave them all the same generic title of LP (all the songs on each have one-word titles, too). Still, dig in closely and subtle modifications and peripheral developments emerge. He’s made some movement toward a denser, busier mix. That trend began on last year’s Adhesive EP—perhaps the best distillation so far of his vision—and it continues during this album’s quick, 27-minute sprint.The moments where things sound like they’re spilling over, bleeding outside of the track's imaginary lines, are when LP is most thrilling. The overlapping drum-machines in opener \"Eject\" give the sense that Schofield is discovering the beats as he makes them, a tone matched by the found-footage video constructed by Valerie Martino (aka noise-beat-maker Unicorn Hard-on). \"Cushion\" starts like a straight dance track, but eventually metallic whirrs and siren calls pile on like rugby players in a scrum. The peak is closer \"Calibrate\", a hypnotic mess of stark hip-hop rhythm and hyperactive static.All of the above tracks drive firmly in Container’s established lane. It will be interesting to see if Schofield ever feels compelled to steer his rumbling vehicle into some unexpected left turns. His omnivorous creativity—a clear openness to all types of sounds—suggests he could carve a new path without losing sight of the one he’s already crafted. But whether or not that ever happens, what he’s done with Container so far is enough to ride out for a long time.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pipeline = make_pipeline(vectorizer, model20)\n",
    "\n",
    "def rank_topics(topics, vec):\n",
    "    for i in np.argsort(-vec)[:5]:\n",
    "        print '%.3f: %s' % (vec[i], topics[i])\n",
    "\n",
    "example = df.iloc[np.random.randint(len(df))]\n",
    "        \n",
    "top_vec = pipeline.transform([example.content])\n",
    "rank_topics(topics20, top_vec[0])\n",
    "\n",
    "print\n",
    "print '\"%s\" by %s (%s)' % (example.title, example.artist, example.genre)\n",
    "print\n",
    "print example.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
